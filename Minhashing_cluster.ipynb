{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ef487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f8a67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2968263, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_minhashing=pd.read_csv(\"/Users/sararedaelli/Desktop/AMAZON REVIEWS/minhashing_results.csv\")\n",
    "df=data_minhashing\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b4e076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['signature', 'rating'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81db87a",
   "metadata": {},
   "source": [
    "## CLUSTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769de78",
   "metadata": {},
   "source": [
    "# Each cell contains an alternative way to do clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01ba17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#KMEANS with minibatch (FASTER THAN NORMAL K-MEANS)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# --- 1. Prepare data ---\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Assuming df['signature'] contains lists or arrays of length 100\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m X = np.array(\u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33msignature\u001b[39m\u001b[33m'\u001b[39m].tolist())\n\u001b[32m      5\u001b[39m y = df[\u001b[33m'\u001b[39m\u001b[33mrating\u001b[39m\u001b[33m'\u001b[39m].values  \u001b[38;5;66;03m# true labels (optional, per valutazione)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- 2. Run Mini-Batch K-Means ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#KMEANS with minibatch (FASTER THAN NORMAL K-MEANS)\n",
    "# --- 1. Prepare data ---\n",
    "# Assuming df['signature'] contains lists or arrays of length 100\n",
    "X = np.array(df['signature'].tolist())\n",
    "y = df['rating'].values  # true labels (optional, per valutazione)\n",
    "\n",
    "# --- 2. Run Mini-Batch K-Means ---\n",
    "mini_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=5,\n",
    "    random_state=42,\n",
    "    batch_size=100,     \n",
    "    n_init='auto',      \n",
    "    max_iter=1000       \n",
    ")\n",
    "\n",
    "clusters = mini_kmeans.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72636abb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#K-MEANS  \n",
    "# --- 1. Prepare data ---\n",
    "# Assuming df['signature'] contains lists or arrays of length 100\n",
    "X = np.array(df['signature'].tolist())\n",
    "y = df['rating'].values  # true labels\n",
    "\n",
    "# --- 2. Run K-Means ---\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "Kmeans_clusters = kmeans.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed135532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-MEANS after PCA  (reduce dimensionality before doing clustering)\n",
    "# --- 1. Prepare data ---\n",
    "# Assuming df['signature'] contains lists or arrays of length 100\n",
    "X = np.array(df['signature'].tolist())\n",
    "\n",
    "# --- 2. PCA: keep enough components to explain 90% of variance ---\n",
    "pca = PCA(n_components=0.9, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"Number of PCA components selected: {pca.n_components_}\")\n",
    "print(f\"Cumulative explained variance: {np.sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# --- 3. Run K-Means on PCA-reduced data ---\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e72eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN AFTER PCA\n",
    "# --- 1. Prepare data ---\n",
    "X = np.array(df['signature'].tolist())\n",
    "\n",
    "# --- 2. PCA: keep 90% cumulative variance ---\n",
    "pca = PCA(n_components=0.9, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"Number of PCA components selected: {pca.n_components_}\")\n",
    "print(f\"Cumulative explained variance: {np.sum(pca.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# --- 3. Estimate a good eps using the k-distance plot ---\n",
    "min_samples = 3\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(X_pca)\n",
    "distances, indices = neighbors_fit.kneighbors(X_pca)\n",
    "\n",
    "# Sort distances for the k-distance graph\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(distances)\n",
    "plt.title(\"k-distance graph (use the 'elbow' to choose eps)\")\n",
    "plt.xlabel(\"Points sorted by distance\")\n",
    "plt.ylabel(f\"{min_samples}-NN distance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ðŸ‘‰ Look at the plot and choose an eps value where the curve sharply increases.\n",
    "# For example, eps = 0.5 (youâ€™ll set it after visual inspection)\n",
    "eps = 0.5  # <-- adjust this after checking the plot visually\n",
    "\n",
    "# --- 4. Run DBSCAN ---\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "clusters = dbscan.fit_predict(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8ec9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DB-SCAN\n",
    "# --- 1. Prepare data ---\n",
    "# Assuming df['signature'] contains lists or arrays of length 100\n",
    "X = np.array(df['signature'].tolist())\n",
    "\n",
    "# --- 2. Estimate a good eps using the k-distance plot ---\n",
    "min_points = 3\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(X)\n",
    "distances, indices = neighbors_fit.kneighbors(X)\n",
    "\n",
    "# Sort distances for the k-distance graph\n",
    "distances = np.sort(distances[:, -1])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(distances)\n",
    "plt.title(\"k-distance graph (use the 'elbow' to choose eps)\")\n",
    "plt.xlabel(\"Points sorted by distance\")\n",
    "plt.ylabel(f\"{min_samples}-NN distance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb07e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the plot and choose an eps value around the 'elbow'\n",
    "eps = 0.5  # <-- adjust after checking the curve\n",
    "\n",
    "# --- 3. Run DBSCAN on the original data (no PCA) ---\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "Dbscan_clusters = dbscan.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283f90d",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b113ee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION using DAVIES-BOULDIN INDEX\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "# --- 3. Evaluate with Davies-Bouldin Index ---\n",
    "db = davies_bouldin_score(X, clusters)\n",
    "print(f\"Davies-Bouldin Index (DB): {db:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68050bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT IN 2D and COLOR by clusters \n",
    "# PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot with legend\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                      c=clusters, cmap='tab10', alpha=0.7)\n",
    "\n",
    "# Add legend with cluster IDs\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.legend(handles, [f\"Cluster {i}\" for i in range(5)], title=\"Clusters\")\n",
    "\n",
    "plt.title(\"K-Means Clustering (PCA 2D)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT IN 2D and COLOR by ratings \n",
    "# Prepare data\n",
    "X = np.array(df['signature'].tolist())  # shape (n_samples, 100)\n",
    "y = df['rating'].values                 # true labels\n",
    "\n",
    "# PCA to 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot colored by rating\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                      c=y, cmap='tab10', alpha=0.7)\n",
    "\n",
    "# Add legend for unique ratings\n",
    "handles, labels = scatter.legend_elements()\n",
    "plt.legend(handles, [f\"Rating {r}\" for r in np.unique(y)], title=\"Ratings\")\n",
    "\n",
    "plt.title(\"PCA Visualization Colored by Rating\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3defd6f0",
   "metadata": {},
   "source": [
    "## COMPARING CLUSTER LABELS WITH ORIGINAL RATINGS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Combine into one DataFrame\n",
    "df_results = pd.DataFrame({\n",
    "    'rating': y,\n",
    "    'cluster': clusters\n",
    "})\n",
    "\n",
    "# Cross-tabulation to see the distribution of ratings per cluster\n",
    "cluster_summary = pd.crosstab(df_results['cluster'], df_results['rating'])\n",
    "print(cluster_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae74254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_rating = cluster_summary.idxmax(axis=1)\n",
    "print(cluster_to_rating)\n",
    "\n",
    "# Map each sampleâ€™s cluster to its assigned rating\n",
    "predicted_rating = df_results['cluster'].map(cluster_to_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030eac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFUSION MATRIX \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y, predicted_rating, labels=np.unique(y))\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Cluster-assigned Rating\")\n",
    "plt.ylabel(\"True Rating\")\n",
    "plt.title(\"Comparison of True vs Cluster-assigned Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bcdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCATTER PLOT \n",
    "plt.scatter(y, predicted_rating, alpha=0.6)\n",
    "plt.xlabel(\"True Rating\")\n",
    "plt.ylabel(\"Cluster-assigned Rating\")\n",
    "plt.title(\"True vs Cluster-assigned Ratings\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y, predicted_rating, alpha=0.6)\n",
    "plt.xlabel(\"True Rating\")\n",
    "plt.ylabel(\"Cluster-assigned Rating\")\n",
    "plt.title(\"True vs Cluster-assigned Ratings\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
