{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409e4260",
   "metadata": {},
   "source": [
    "### Amazon Book Review\n",
    "\n",
    "Introduction balallala\n",
    "\n",
    "\n",
    "0. Data loading\n",
    "1. Preprocessing\n",
    "1.1 Preprocessing Minhashing\n",
    "\n",
    "2. Create Embeddings\n",
    "2.1 Minhashing\n",
    "2.2 SBert\n",
    "\n",
    "3. Clustering for Minhashing\n",
    "3.1 K-Means\n",
    "3.2 HDBscan with UMAP\n",
    "\n",
    "4. Clustering with SBert\n",
    "4.1 Preprocessing for Cluster\n",
    "4.2 K-Means\n",
    "4.3 HDBSCAN\n",
    "\n",
    "5. Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc106ccd",
   "metadata": {},
   "source": [
    "#### 0. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import html\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path_book_reviews = \"Books_rating.csv\" #### SET THE PATH\n",
    "br = pd.read_csv(path_book_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e895f",
   "metadata": {},
   "source": [
    "#### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb17388f",
   "metadata": {},
   "source": [
    "##### 1.1 Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947851d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter columns\n",
    "br = br[['Id','review/score', 'review/text']]\n",
    "#drop NA values\n",
    "br = br.dropna(subset=['review/text', 'review/score'])\n",
    "#unescape html characters\n",
    "br['cleanText'] = br['review/text'].astype(str).apply(html.unescape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2621a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combined cleaning function ---\n",
    "def clean_text_all(text):\n",
    "    text = str(text)\n",
    "\n",
    "    # 1. Remove control characters (non-printable ASCII)\n",
    "    # \\x00-\\x1F are control chars, \\x7F is DEL\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F]', '', text)\n",
    "\n",
    "    # 2. Remove URLs (http/https/www)\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "    # 3. Replace newlines, carriage returns, tabs with a single space\n",
    "    text = re.sub(r'[\\r\\n\\t]+', ' ', text)\n",
    "\n",
    "    # 4. Normalize multiple spaces to one and strip leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# --- Apply to your dataframe ---\n",
    "br['cleanText'] = br['cleanText'].apply(clean_text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'cleanText' is empty after cleaning\n",
    "br = br.dropna(subset=['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that contain non-ASCII characters\n",
    "# Create a boolean mask for rows that contain any non-ASCII characters\n",
    "mask_non_ascii = br['cleanText'].str.contains(r'[^\\x00-\\x7F]', regex=True)\n",
    "\n",
    "# Print how many will be dropped\n",
    "print(f\"Rows containing non-ASCII characters: {mask_non_ascii.sum()}\")\n",
    "\n",
    "# Drop those rows\n",
    "br_clean = br[~mask_non_ascii].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba4778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \\' with ' and \\\" with \" \"\n",
    "br_clean['cleanText'] = br_clean['cleanText'].str.replace(r\"\\\\'\", \"'\", regex=True)\n",
    "br_clean['cleanText'] = br_clean['cleanText'].str.replace(r'\\\\\"', '\"', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_clean[['Id', 'review/score', 'cleanText']].to_csv('Books_rating_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514eba49",
   "metadata": {},
   "source": [
    "##### 1.2 Preprocessing Minhashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50759e2d",
   "metadata": {},
   "source": [
    "#### 2. Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621cd97",
   "metadata": {},
   "source": [
    "##### 2.1 Embeddings Minhasing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb1264",
   "metadata": {},
   "source": [
    "##### 2.2 Embeddings SBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ad6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_arrow_table(ids, texts, score, embs, sent_scores):\n",
    "    arr_ids = pa.array(ids)\n",
    "    arr_texts = pa.array(texts, type=pa.string())\n",
    "    arr_scores = pa.array(score)\n",
    "    list_of_lists = [emb.tolist() for emb in embs]\n",
    "    arr_embs = pa.array(list_of_lists, type=pa.list_(pa.float32()))\n",
    "    \n",
    "    arr_sent = pa.array(sent_scores.tolist(), type=pa.float32())\n",
    "    return pa.Table.from_arrays(\n",
    "        [arr_ids, arr_scores, arr_texts, arr_embs, arr_sent],\n",
    "        names=[\"row_id\", \"review/score\", \"cleanText\", \"embedding\", \"sentiment_score\"]\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment_scores(texts, tokenizer, model, device, batch_size=64, max_length=256):\n",
    "\n",
    "    scores = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[start:start + batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=max_length\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            outputs = model(**enc)\n",
    "            probs = F.softmax(outputs.logits, dim=1)  # shape: [B, 5] (for 1â€“5 stars)\n",
    "\n",
    "            stars = torch.arange(1, probs.size(1) + 1, device=device, dtype=torch.float32)\n",
    "            batch_scores = (probs * stars).sum(dim=1) \n",
    "            scores.extend(batch_scores.cpu().numpy().tolist())\n",
    "\n",
    "    return np.array(scores, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c5e09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m sent_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m      4\u001b[0m sent_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create SBERT and Sentiment Analysis models\n",
    "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\" )\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1239483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv(path_book_reviews, sep=\",\", encoding=\"utf-8\", dtype=str, keep_default_na=False,on_bad_lines=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de251e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts and scores\n",
    "texts = reader[\"cleanText\"].astype(str).tolist()\n",
    "score = reader[\"Id\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecf641",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.encode(\n",
    "                texts,\n",
    "                convert_to_numpy=True,\n",
    "                show_progress_bar=True\n",
    "            ).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76452661",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = compute_sentiment_scores(\n",
    "                texts,\n",
    "                sent_tokenizer,\n",
    "                sent_model,\n",
    "                batch_size=64,       \n",
    "                max_length=256\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embeddings and sentiment scores and data to arrow table\n",
    "table = as_arrow_table(\n",
    "    ids=reader[\"Id\"].astype(str).tolist(),\n",
    "    texts=reader[\"cleanText\"].astype(str).tolist(),\n",
    "    score=reader[\"review/score\"].astype(str).tolist(),\n",
    "    embs=embs,\n",
    "    sent_scores=sentiment_scores\n",
    ")\n",
    "pq.write_table(table, \"book_reviews_with_embeddings.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5385fa",
   "metadata": {},
   "source": [
    "#### Clustering for Minhashing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26446eb",
   "metadata": {},
   "source": [
    "#### Clustering with SBert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a1183",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
